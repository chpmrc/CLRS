2.1-1)

	On paper.

2.1-2)

	def insertionsortBackwards(arr):
	for i in range(1, len(arr)):
		val = arr[i]
		j = i - 1
		while j >= 0 and arr[j] < val:
			arr[j+1] = arr[j]
			j -= 1

		arr[j+1] = val

	return arr

2.1-3)

	Invariant: the left sub-array doesn't contain our element for sure, the right one might.

	def linearsearch(arr, e):
		index = null
		for i in range(0, arr):
			if arr[i] == e:
				index = i
				break

		return index

	Initialization: The left sub-array is empty, cannot contain the value.
	Maintenance: the left sub-array has already been scanned and it cannot contain the value, the right might.
	Termination: if the right sub-array is empty it means the value is not in the array, otherwise we found it.

2.1-4)

	The problem of storing the sum of two n-bit integers is to compute the carry and change the bit number 0.

	See binarysum.py.

2.2-1)

    O(n^3)

2.2-2)

	Invariant: array [0, i-1] is already sorted.
    Initialization: left sub-array is empty, so it's already sorted.
    Maintenance: the left sub-array is organized as follows: at position i we have the i-th minimum element (in increasing order). So it's sorted. What changes is the current element so this condition is true before and after each iteration.
    Termination: the right sub-array is empty so cannot contain any value that is less than the ones already scanned, the left sub-array is the complete array and it's sorted.

    It only needs to run for n-1 elements because the last element contains the greatest element (after the last exchange).

    Best case analysis:
    	The array is already sorted. The total (approx.) running time is T(n) = (n - 1)^2 = Ø(n^2)
    	We need to search if there is a minimum anyway so we perform a linear search at each iteration. At iteration i we have to scan (n - i) elements.

    Worst case analysis:
    	The array is sorted in the opposite way. The total (approx.) running time is T(n) = (n-1)^2 = Ø(n^2), same as above (linear search required!).

2.2-3)

	Average case: n/2.
	Worst case: n.

	Average case and worst case in Ø notation are the same: Ø(n) because we treat 1/2 as a constant.

2.2-4)

	I found it online, it's not fair. LOL

2.3-1)

		[3, 41, 52, 26, 38, 57, 9, 49]
		[3, 41, 52, 26] [38, 57, 9, 49]
		[3, 41] [52, 26] [38, 57] [9, 49]
		[3] [41] [52] [26] [38] [57] [9] [49]

		[3, 41], [26, 52], [38, 57], [9, 49]
		[3, 26, 41, 52], [9, 38, 49, 57]
		[3, 26, 38, 41, 49, 52, 57]

2.3-2)

	See mergesort.py

2.3-3)

	Base case: T(2) = 2, T(4) = 2T(2) + 4 = 8 = 4 * (lg(4)) - OK
	Inductive case:
		Let's suppose T(i) = i * lg(i)
		T(i + 1) = (i + 1) * lg(i + 1) ? // With i + 1 = 2^k
		T(i + 1) =
			2 * T((i + 1) / 2) + (i + 1) =
			2 * (2 * T((i + 1) / 4) + (i + 1)) + (i + 1) =
			... =
			2^lg(i + 1)+ lg(i + 1)(i + 1) =
			(i + 1) + (i + 1) * lg(i + 1) // This may be correct...

2.3-4)

	See insertionsort-rec.py

	T(n) = T(n-1) + (n-1) = sum{0, n-1}(n)

2.3-5)

	See binarysearch.py

	T(n) = T(n/2) = lg(n)

2.3-6)

	No, we can't since the sub-array is not necessarily sorted (which is a pre-condition to use binarysearch).

2.3-7)

	See containssum.py

=== Problems

2.1)

	a. Recurrence of insertionsort is T(n) = n * (n - 1). In case of a list of size k we have T(k) = k + (k - 1) ~= k^2. Summing up all the recurrences we have sum{0, n}(k^2) = k^2 + k^2 + ... + k^2 = k(k + k + k + ... + k) = k * n

	b. Since merging remains of the same complexity O(n) we are interested in the division process. Simply enough at level n/k we traversed a total of lg(n/k) levels (since in mergesort last level is n/1 = n we have lg(n)). The process is exactly the same, except we don't go up having lists of 1 elements.

	c.
		lg(n) * n - (n * k + n * lg(n / k)) = 0

	d.
		*Missing*

2.2)
	a.
		That we start from an unsorted array?
	b.
		Invariant: The element in A[j] is the smallest of A[j..n]

		Initialization: A[j..n] consists of only one element, trivial.
		Maintenance: before the possible swap A[j] holds the minimum of A[j..n]. If there is a swap it

		i   j
		4 2 3

        i j
		4 2 3

        swap!
		2 4 3 -> But 4 is not smaller than 3 so the invariant is WRONG! Let's find another one...

		Invariant: the subarray A[i..j] contains the smallest element of the subarray A[i..n]

		Initialization: A[i..j] = A[i..n] so yeah, "trivial".
		Maintenance: the only modification in the inner loop is the swap of A[j] and A[j-1] and the array A[i..j] still contains the smallest
		Termination: the loop terminates when j = i + 1. After all the swaps the minimum element in A[i..j] will be exactly in A[i+1] so the invariant still holds.

	c.
		Invariant: the array A[1..i] contains the original elements but sorted

		Initialization: A[1..1] is trivially sorted.
		Maintenance: since at each iteration the minimum element in A[i..j] is moved to A[i] the invariant holds.
		Termination: at the end i = A.length so A[1..A.length] is completely sorted.

	d.
		The worst case is the same as the best and average case (because the elements have to be checked even if not swapped).
		c(n - 1) * sum{i=1,n}(n - i) * 2c = c(n - 1) * n * (n - 1) / 2 * 2c = Ø(n^2)
		The complexity is the same as insertionsort (except in the best case where insertionsort is Ø(n)).

2-3)
	a.
		Ø(n)
	b.
		result = 0
		for i = 0 to n
			result += ai * x ^ i // i + 1 multiplications

		In terms of Ø notation they have the same complexity but Horner's algorithm is faster with constants (only 1 multiplication per step)

	c.
		Initialization: given in the exercise as an axiom.
		Maintenance: given.
		Termination: terminates when i = -1. Replacing i with -1 in the summation we get the one on the book.

	d.
		We can see that from the formula got from the proof in point c.

2-4)

	a.
		(1,5), (2,5), (3,4), (3,5) and (4,5)
	b.
		The array with decreasing values {n, n-1, n-2, ..., 2, 1}. It has n(n-1) / 2 inversions.
	c.
		In the worst case of insertion sort (inverted array) the running time is Ø(n^2). Same order of the number of inversions.
	d.
		In the merge function whenever we pick an element from the right array we increase the number of inversions by 1.
